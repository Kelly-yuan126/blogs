# 论文梳理

- ### Abstract

  - We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences
  - Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence
  - 做了什么事情：The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations
  - 目标是什么：We propose to fight **the curse of dimensionality** by learning a distributed representation for words which allows each training sentence to inform the model about an exponential(指数) number of semantically(在语义上) neighboring sentences。
  - 论文概述：We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts。

- ### Introduction

  - A statistical model of language can be represented by the conditional probability of the next word given all the previous ones, since

    ​										![image-20211221134756670](C:\Users\wangyuan\AppData\Roaming\Typora\typora-user-images\image-20211221134756670.png)

  - 注解：

  - When building statistical models of natural language, one considerably reduces the difficulty
    of this modeling problem by taking advantage of word order, and the fact that temporally closer
    words in the word sequence are statistically more dependent.

  - 注解：

  - Thus, n-gram models construct tables of conditional probabilities for the next word, for each one of a large number of contexts, i.e. combinations of the last n−1 words:

    ![image-20211221172621507](C:\Users\wangyuan\AppData\Roaming\Typora\typora-user-images\image-20211221172621507.png)

  - 注解：

  - Obviously there is much more information in the sequence that immediately precedes the word to predict than just the identity of the previous couple of words.

  - 注解：back-off trigram:三元语言模型

    - [x] **马尔可夫假设**

    在语言模型中，马克科夫假设即下一个词的出现仅仅依赖于它前面的一个或者几个词。但此方法具有一定的局限性，忽略了语言中的长依赖。

    - 如果一个词的出现与它周围的词是独立的，则称之为**一元语言模型(unigram)**

    ![[公式]](https://www.zhihu.com/equation?tex=P%28w_%7B1%7D%2C+w_%7B2%7D%2C+w_%7B3%7D%2C+...%2C+w_%7Bn%7D%29+%3D+P%28w_%7B1%7D%29+%2A+P%28w_%7B2%7D%29+%2A+P%28w_%7B3%7D%29+%2A+...+%2A+P%28w_%7Bn%7D%29)

    - 如果一个词的出现仅依赖于它前面出现的一个词，则称之为**二元语言模型(bigram)**

    ![[公式]](https://www.zhihu.com/equation?tex=P%28w_%7B1%7D%2C+w_%7B2%7D%2C+w_%7B3%7D%2C+...%2C+w_%7Bn%7D%29+%3D+P%28w_%7B1%7D%29+%2A+P%28w_%7B2%7D%7Cw_%7B1%7D%29+%2A+P%28w_%7B3%7D%7Cw_%7B2%7D%29+%2A+...+%2A+P%28w_%7Bn%7D%7Cw_%7Bn-1%7D%29)

    - 如果一个词的出现仅依赖于它前面出现的两个词，则称之为**三元语言模型(trigram)**

    ![[公式]](https://www.zhihu.com/equation?tex=P%28w_%7B1%7D%2C+w_%7B2%7D%2C+w_%7B3%7D%2C+...%2C+w_%7Bn%7D%29+%3D+P%28w_%7B1%7D%29+%2A+P%28w_%7B2%7D%7Cw_%7B1%7D%29+%2A+P%28w_%7B3%7D%7Cw_%7B2%7D%2Cw_%7B1%7D%29+%2A+...+%2A+P%28w_%7Bn%7D%7Cw_%7Bn-1%7D%2Cw_%7Bn-2%7D%29)

  